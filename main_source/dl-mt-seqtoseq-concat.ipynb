{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resize_images(input_dir, output_dir, size=(224, 224)):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for img_file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        save_path = os.path.join(output_dir, img_file)\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize(size, Image.LANCZOS) \n",
    "                img.save(save_path, img.format)\n",
    "        except Exception as e:\n",
    "            print(f\"Wrong at {img_file}: {e}\")\n",
    "\n",
    "    print(f\"Resized: {len(os.listdir(output_dir))} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/train\"\n",
    "output_dir = \"/kaggle/working/resized_images_train\"\n",
    "resize_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/test\"\n",
    "output_dir = \"/kaggle/working/resized_images_test\"\n",
    "resize_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    regex = re.compile(r'(\\W+)')\n",
    "    tokens = regex.split(sentence.lower())\n",
    "    return [w.strip() for w in tokens if len(w.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_ans_vocab(annotation_file, save_path=\"/kaggle/working/answer_vocabs.txt\"):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    answers = set()\n",
    "    for ann in data['annotations']:\n",
    "        for ans in ann['answers']:\n",
    "            answers.update(tokenizer(ans['answer']))  \n",
    "\n",
    "    answers = sorted(answers)\n",
    "    answers.insert(0, '<pad>') \n",
    "    answers.insert(1, '<unk>') \n",
    "    answers.insert(2, '<sos>') \n",
    "    answers.insert(3, '<eos>') \n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines([a + '\\n' for a in answers])\n",
    "\n",
    "    print(f\"Generated answer vocab: {len(answers)} answers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "annotation_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/train_annotations.json\"\n",
    "make_ans_vocab(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_q_vocab(question_file, save_path=\"/kaggle/working/question_vocabs.txt\"):\n",
    "    with open(question_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    words = set()\n",
    "    for q in data['questions']:\n",
    "        words.update(tokenizer(q['question']))\n",
    "\n",
    "    words = sorted(words)\n",
    "    words.insert(0, '<pad>')\n",
    "    words.insert(1, '<unk>')\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines([w + '\\n' for w in words])\n",
    "\n",
    "    print(f\"Generated question vocab: {len(words)} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/train_questions.json\"\n",
    "make_q_vocab(question_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "    return {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_image_file(image_id, image_dir):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.startswith(f\"id_{image_id}.\"):\n",
    "            return os.path.join(image_dir, filename) \n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_data(question_file, annotation_file, image_dir, output_path, labeled=True):\n",
    "    with open(question_file, 'r', encoding=\"utf-8\") as f:\n",
    "        questions = json.load(f)['questions']\n",
    "\n",
    "    if labeled:\n",
    "        with open(annotation_file, 'r', encoding=\"utf-8\") as f:\n",
    "            annotations = json.load(f)['annotations']\n",
    "        q_dict = {ann['question_id']: ann for ann in annotations}\n",
    "\n",
    "    vocab2idx = load_vocab(\"/kaggle/working/question_vocabs.txt\")  \n",
    "    ans_vocab2idx = load_vocab(\"/kaggle/working/answer_vocabs.txt\")  \n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for q in questions:\n",
    "        qu_id = q['question_id']\n",
    "        img_id = q['image_id']\n",
    "        qu_sentence = q['question']\n",
    "        qu_tokens = tokenizer(qu_sentence)\n",
    "        qu2idx = [vocab2idx.get(token, vocab2idx['<unk>']) for token in qu_tokens]  \n",
    "\n",
    "        img_path = find_image_file(img_id, image_dir)\n",
    "\n",
    "        info = {\n",
    "            'img_name': os.path.basename(img_path),\n",
    "            'img_path': img_path,\n",
    "            'question': qu_sentence,\n",
    "            'qu_tokens': qu2idx,\n",
    "            'qu_id': qu_id\n",
    "        }\n",
    "\n",
    "        if labeled:\n",
    "            annotation_ans = q_dict[qu_id]['answers']\n",
    "            valid_ans = [ans['answer'] for ans in annotation_ans]\n",
    "            ans_tokens = tokenizer(valid_ans[0])\n",
    "            ans2idx = [ans_vocab2idx.get(token, ans_vocab2idx['<unk>']) for token in ans_tokens]\n",
    "            \n",
    "            info['answer'] = valid_ans[0]\n",
    "            info['ans_tokens'] = ans2idx \n",
    "            \n",
    "        dataset.append(info)\n",
    "\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4, ensure_ascii=False)  \n",
    "\n",
    "    print(f\"Saved JSON at {output_path} | Total samples: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_dir = \"/kaggle/working/resized_images_train\"\n",
    "question_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/train_questions.json\"\n",
    "annotation_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/train_annotations.json\"\n",
    "\n",
    "process_data(question_file, annotation_file, image_dir, '/kaggle/working/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_dir = \"/kaggle/working/resized_images_test\"\n",
    "question_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/test_questions.json\"\n",
    "annotation_file = \"/kaggle/input/dl-mt-sq2sq-data/data_seq2seq/test_annotations.json\"\n",
    "\n",
    "process_data(question_file, annotation_file, image_dir, '/kaggle/working/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, json_path, transform=None,\n",
    "                 max_qu_len=20, max_ans_len=20,\n",
    "                 typeData='train',\n",
    "                 pad_idx=0, sos_idx=2, eos_idx=3):\n",
    "        \n",
    "        self.max_qu_len = max_qu_len\n",
    "        self.max_ans_len = max_ans_len\n",
    "        self.typeData = typeData\n",
    "        \n",
    "        self.PAD_IDX = pad_idx\n",
    "        self.SOS_IDX = sos_idx\n",
    "        self.EOS_IDX = eos_idx\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        if self.typeData == 'train':\n",
    "            for i in range(0, len(data), 210):\n",
    "                self.data.extend(data[i:i+200]) \n",
    "        elif self.typeData == 'valid':\n",
    "            for i in range(0, len(data), 210):\n",
    "                self.data.extend(data[i+200:i+210]) \n",
    "        elif self.typeData == 'test':\n",
    "            self.data = data \n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img_path = sample[\"img_path\"]\n",
    "        question_tokens = torch.tensor(sample[\"qu_tokens\"], dtype=torch.long)\n",
    "        answer_tokens = torch.tensor(sample[\"ans_tokens\"], dtype=torch.long)\n",
    "\n",
    "        #\n",
    "        if len(question_tokens) > self.max_qu_len:\n",
    "            question_tokens = question_tokens[:self.max_qu_len]\n",
    "        else:\n",
    "            pad_len = self.max_qu_len - len(question_tokens)\n",
    "            padding = torch.full((pad_len,), self.PAD_IDX, dtype=torch.long)\n",
    "            question_tokens = torch.cat([question_tokens, padding])\n",
    "\n",
    "        \n",
    "        if len(answer_tokens) > self.max_ans_len - 2:\n",
    "            answer_tokens = answer_tokens[:self.max_ans_len - 2]\n",
    "\n",
    "        \n",
    "        answer_tokens = torch.cat([\n",
    "            torch.tensor([self.SOS_IDX], dtype=torch.long),\n",
    "            answer_tokens,\n",
    "            torch.tensor([self.EOS_IDX], dtype=torch.long)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        if len(answer_tokens) < self.max_ans_len:\n",
    "            pad_len = self.max_ans_len - len(answer_tokens)\n",
    "            padding = torch.full((pad_len,), self.PAD_IDX, dtype=torch.long)\n",
    "            answer_tokens = torch.cat([answer_tokens, padding])\n",
    "\n",
    "        \n",
    "        answer_input = answer_tokens[:-1]   \n",
    "        answer_target = answer_tokens[1:]   \n",
    "\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, question_tokens, answer_input, answer_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self, model, in_features, img_feature_size, is_train=False):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(in_features, img_feature_size)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def forward(self, image):\n",
    "        if self.is_train:\n",
    "            img_feature = self.model(image)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                img_feature = self.model(image)\n",
    "\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = F.normalize(img_feature, p=2, dim=1)\n",
    "        return l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1) \n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)  \n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1) \n",
    "        return context_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QuEncoder(nn.Module):\n",
    "    def __init__(self, qu_vocab_size, word_embed, hidden_size, num_hidden, qu_feature_size, with_att=False):\n",
    "        super(QuEncoder, self).__init__()\n",
    "        self.with_att = with_att\n",
    "        self.word_embedding = nn.Embedding(qu_vocab_size, word_embed)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(word_embed, hidden_size, num_hidden, batch_first=True)\n",
    "\n",
    "        if self.with_att:\n",
    "            self.attention = SelfAttention(hidden_size)\n",
    "            self.fc = nn.Linear(hidden_size, qu_feature_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2 * num_hidden * hidden_size, qu_feature_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qu_embedding = self.word_embedding(question)\n",
    "        qu_embedding = self.tanh(qu_embedding)\n",
    "        lstm_out, (hidden, cell) = self.lstm(qu_embedding)\n",
    "    \n",
    "        if self.with_att:\n",
    "            attn_output = self.attention(lstm_out)  \n",
    "        else:\n",
    "            qu_feature = torch.cat((hidden, cell), dim=2)\n",
    "            qu_feature = qu_feature.transpose(0, 1).reshape(qu_feature.size(1), -1)\n",
    "            attn_output = self.tanh(qu_feature)\n",
    "    \n",
    "        qu_feature = self.fc(attn_output)  \n",
    "    \n",
    "        return attn_output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AnswerDecoder(nn.Module):\n",
    "    def __init__(self, ans_vocab_size, word_embed, hidden_size, num_layers=1, pad_idx=0):\n",
    "        super(AnswerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(ans_vocab_size, word_embed, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(word_embed, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, ans_vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, answer_input, hidden):\n",
    "        embedded = self.embedding(answer_input)  \n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  \n",
    "        outputs = self.fc(lstm_out)  \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## VQA MODEL ##\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, img_model, img_in_features,\n",
    "                 qu_vocab_size, ans_vocab_size,\n",
    "                 word_embed, hidden_size, num_hidden,\n",
    "                 feature_size, with_att=False,\n",
    "                 is_train_image=False, pad_idx=0):\n",
    "        super(VQAModel, self).__init__()\n",
    "\n",
    "        if (with_att):\n",
    "            output_size = 768\n",
    "        else: \n",
    "            output_size = 1536\n",
    "        \n",
    "        \n",
    "        img_model = img_model.to(device)\n",
    "        self.img_encoder = ImgEncoder(img_model, img_in_features, feature_size, is_train_image)\n",
    "        self.qu_encoder = QuEncoder(qu_vocab_size, word_embed, hidden_size, num_hidden, feature_size, with_att)\n",
    "        self.feature_projection = nn.Linear(output_size, hidden_size)\n",
    "        self.decoder = AnswerDecoder(ans_vocab_size, word_embed, hidden_size, pad_idx=pad_idx)\n",
    "\n",
    "    def forward(self, image, question, answer_input):\n",
    "        img_feature = self.img_encoder(image)\n",
    "        qu_feature, hidden, cell = self.qu_encoder(question)\n",
    "        combined_feature = torch.cat([img_feature, qu_feature], dim=1) \n",
    "        projected_feature = torch.tanh(self.feature_projection(combined_feature))\n",
    "        h0 = projected_feature.unsqueeze(0)\n",
    "        c0 = torch.tanh(cell[-1]).unsqueeze(0)\n",
    "        outputs = self.decoder(answer_input, (h0, c0))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, history['valid_loss'], label='Valid Loss', marker='o')\n",
    "    \n",
    "    plt.title(\"Loss theo Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader,\n",
    "          criterion, optimizer, device,\n",
    "          num_epochs=10, name=\"default\", patience=10):\n",
    "\n",
    "    model.to(device)\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'valid_loss': []}\n",
    "\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, questions, answer_input, answer_target in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            questions = questions.to(device)\n",
    "            answer_input = answer_input.to(device)\n",
    "            answer_target = answer_target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions, answer_input)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                             answer_target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, questions, answer_input, answer_target in valid_dataloader:\n",
    "                images = images.to(device)\n",
    "                questions = questions.to(device)\n",
    "                answer_input = answer_input.to(device)\n",
    "                answer_target = answer_target.to(device)\n",
    "\n",
    "                outputs = model(images, questions, answer_input)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
    "                                 answer_target.view(-1))\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_dataloader)\n",
    "\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]: \"\n",
    "              f\"Train Loss = {train_loss:.4f} | \"\n",
    "              f\"Valid Loss = {valid_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'valid_loss': valid_loss\n",
    "        }, f\"last_{name}.pt\")\n",
    "\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_loss': valid_loss\n",
    "            }, f\"best_{name}.pt\")\n",
    "\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {valid_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}, no improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, dataloader, ans_vocab_file, device, max_len=20):\n",
    "    ans_word_to_idx = load_vocab(ans_vocab_file)\n",
    "    \n",
    "    ans_idx_to_word = {idx: word for word, idx in ans_word_to_idx.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    references = []  \n",
    "    hypotheses = []  \n",
    "    \n",
    "    \n",
    "    start_token = ans_word_to_idx.get(\"<sos>\", 2)\n",
    "    end_token = ans_word_to_idx.get(\"<eos>\", 3)\n",
    "    pad_token = ans_word_to_idx.get(\"<pad>\", 0)\n",
    "    \n",
    "    smooth = SmoothingFunction().method4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, questions, _, answers_gt in dataloader:\n",
    "            images = images.to(device)\n",
    "            questions = questions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            \n",
    "            answer_input = torch.full((batch_size, 1), start_token, dtype=torch.long).to(device)\n",
    "            \n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                outputs = model(images, questions, answer_input)  \n",
    "                next_token = torch.argmax(outputs[:, -1, :], dim=-1).unsqueeze(1)  \n",
    "                answer_input = torch.cat([answer_input, next_token], dim=1)\n",
    "            \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                pred_ids = answer_input[i, 1:].tolist()\n",
    "                pred_words = []\n",
    "                for idx in pred_ids:\n",
    "                    word = ans_idx_to_word.get(idx, \"<unk>\")\n",
    "                    if word in (\"<eos>\", \"<pad>\"):\n",
    "                        break\n",
    "                    pred_words.append(word)\n",
    "                hypotheses.append(pred_words)\n",
    "                \n",
    "                \n",
    "                gt_ids = answers_gt[i].tolist()\n",
    "                gt_words = []\n",
    "                for idx in gt_ids:\n",
    "                    word = ans_idx_to_word.get(idx, \"<unk>\")\n",
    "                    if word in (\"<eos>\", \"<pad>\"):\n",
    "                        break\n",
    "                    gt_words.append(word)\n",
    "                \n",
    "                references.append([gt_words])\n",
    "                \n",
    "    bleu = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "    print(f\"BLEU score: {bleu:.4f}\")\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "json_path_train = \"/kaggle/working/train.json\"  \n",
    "json_path_test = \"/kaggle/working/test.json\"  \n",
    "\n",
    "train_dataset = VQADataset(json_path_train, max_qu_len=20, typeData='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "valid_dataset = VQADataset(json_path_train, max_qu_len=20, typeData='valid')\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = VQADataset(json_path_test, max_qu_len=20, typeData='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for images, questions, answers_input, answers_target in train_dataloader:\n",
    "    for i in range(5):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(\"Image shape:\", images[i].shape)\n",
    "        print(\"Question token IDs:\", questions[i])\n",
    "        print(\"Answer input token IDs:\", answers_input[i])\n",
    "        print(\"Answer target token IDs:\", answers_target[i])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_qu = load_vocab(\"/kaggle/working/question_vocabs.txt\")\n",
    "vocab_ans = load_vocab(\"/kaggle/working/answer_vocabs.txt\")\n",
    "\n",
    "qu_vocab_size = len(vocab_qu)\n",
    "ans_vocab_size = len(vocab_ans)\n",
    "\n",
    "PAD_IDX = vocab_ans[\"<pad>\"]\n",
    "SOS_IDX = vocab_ans[\"<sos>\"]\n",
    "EOS_IDX = vocab_ans[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_size = 512\n",
    "word_embed = 128\n",
    "hidden_size = 256\n",
    "num_hidden = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenet.classifier = nn.Identity() \n",
    "in_features_mobilenet = 1280  \n",
    "\n",
    "mobilenet_model = VQAModel(\n",
    "    img_model=mobilenet,\n",
    "    img_in_features=in_features_mobilenet,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=True,\n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX  \n",
    ")\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(mobilenet_model.parameters(), lr=0.0005, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(mobilenet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='mobile_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_mobile_net.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "mobilenet_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "mobilenet_model.to(device)\n",
    "mobilenet_model.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=mobilenet_model,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet.fc = nn.Identity()\n",
    "in_features_resnet = 2048   \n",
    "\n",
    "resnet_model = VQAModel(\n",
    "    img_model=resnet,\n",
    "    img_in_features=in_features_resnet,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=True,\n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(resnet_model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(resnet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_resnet.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "resnet_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=resnet_model,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "efficientnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "efficientnet.classifier = nn.Identity()\n",
    "\n",
    "in_features_efficientnet = 1536\n",
    "\n",
    "efficientnet_model = VQAModel(\n",
    "    efficientnet, \n",
    "    in_features_efficientnet, \n",
    "    qu_vocab_size, \n",
    "    ans_vocab_size,\n",
    "    word_embed, \n",
    "    hidden_size, \n",
    "    num_hidden, \n",
    "    feature_size,  \n",
    "    with_att=True, \n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(efficientnet_model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(efficientnet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='efficientnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_efficientnet.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "efficientnet_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "efficientnet_model.to(device)\n",
    "efficientnet_model.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=efficientnet_model,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenet.classifier = nn.Identity()  \n",
    "in_features_mobilenet = 1280  \n",
    "\n",
    "mobilenet_model_no_att = VQAModel(\n",
    "    img_model=mobilenet,\n",
    "    img_in_features=in_features_mobilenet,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=False,             \n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(mobilenet_model_no_att.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(mobilenet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='mobile_net_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_mobile_net_no_att.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "mobilenet_model_no_att.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "mobilenet_model_no_att.to(device)\n",
    "mobilenet_model_no_att.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=mobilenet_model_no_att,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet.fc = nn.Identity()\n",
    "in_features_resnet = 2048   \n",
    "\n",
    "resnet_model_no_att = VQAModel(\n",
    "    img_model=resnet,\n",
    "    img_in_features=in_features_resnet,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=False,              \n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(resnet_model_no_att.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(resnet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='resnet_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_resnet_no_att.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "resnet_model_no_att.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "resnet_model_no_att.to(device)\n",
    "resnet_model_no_att.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=resnet_model_no_att,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "efficientnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "efficientnet.classifier = nn.Identity()\n",
    "\n",
    "in_features_efficientnet = 1536\n",
    "\n",
    "efficientnet_model_no_att = VQAModel(\n",
    "    img_model=efficientnet,\n",
    "    img_in_features=in_features_efficientnet,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=False,               \n",
    "    is_train_image=False,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(efficientnet_model_no_att.parameters(), lr=0.0005, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(efficientnet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='efficientnet_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_efficientnet_no_att.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "efficientnet_model_no_att.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "efficientnet_model_no_att.to(device)\n",
    "efficientnet_model_no_att.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=efficientnet_model_no_att,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FromScarth Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNN_Self_Build(nn.Module):\n",
    "    def __init__(self, feature_dim=3096, dropout_rate=0.5):\n",
    "        super(CNN_Self_Build, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, feature_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)   \n",
    "        x = self.block2(x)   \n",
    "        x = self.block3(x)   \n",
    "        x = self.block4(x)   \n",
    "        x = self.global_avg_pool(x)  \n",
    "        x = x.view(x.size(0), -1)      \n",
    "        features = self.fc(x)          \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "in_features_cnn_self_build = 2048\n",
    "\n",
    "cnn_model = VQAModel(\n",
    "    img_model=cnn_self_build,\n",
    "    img_in_features=in_features_cnn_self_build,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=True,                \n",
    "    is_train_image=True,         \n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(cnn_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_default.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "cnn_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=cnn_model,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "in_features_cnn_self_build = 2048\n",
    "\n",
    "cnn_model_no_att = VQAModel(\n",
    "    img_model=cnn_self_build,\n",
    "    img_in_features=in_features_cnn_self_build,\n",
    "    qu_vocab_size=qu_vocab_size,\n",
    "    ans_vocab_size=ans_vocab_size,\n",
    "    word_embed=word_embed,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden=num_hidden,\n",
    "    feature_size=feature_size,\n",
    "    with_att=False,           \n",
    "    is_train_image=True,       \n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(cnn_model_no_att.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# history = train(cnn_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='default_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/input/dl_mt_model/pytorch/default/1/model/seq2seq_concat/best_default_no_att.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "cnn_model_no_att.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "cnn_model_no_att.to(device)\n",
    "cnn_model_no_att.eval()\n",
    "\n",
    "\n",
    "bleu_score = evaluate_bleu(\n",
    "    model=cnn_model_no_att,\n",
    "    dataloader=test_dataloader,     \n",
    "    ans_vocab_file=\"/kaggle/working/answer_vocabs.txt\",\n",
    "    device=device,\n",
    "    max_len=20\n",
    ")\n",
    "\n",
    "print(f\"BLEU Score on test set: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "# in_features_cnn_self_build = 2048\n",
    "\n",
    "# cnn_model_no_att = VQAModel(\n",
    "#     img_model=cnn_self_build,\n",
    "#     img_in_features=in_features_cnn_self_build,\n",
    "#     qu_vocab_size=qu_vocab_size,\n",
    "#     ans_vocab_size=ans_vocab_size,\n",
    "#     word_embed=word_embed,\n",
    "#     hidden_size=hidden_size,\n",
    "#     num_hidden=num_hidden,\n",
    "#     feature_size=feature_size,\n",
    "#     with_att=False,              \n",
    "#     is_train_image=True,       \n",
    "#     pad_idx=PAD_IDX\n",
    "# )\n",
    "\n",
    "# checkpoint_path = '/kaggle/input/dl_demo_df_na/pytorch/default/1/best_default_no_att (1).pt'\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# cnn_model_no_att.load_state_dict(checkpoint['model_state_dict'])\n",
    "# cnn_model_no_att.to(device)\n",
    "# cnn_model_no_att.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "# mobilenet.classifier = nn.Identity()\n",
    "# in_features_mobilenet = 1280\n",
    "\n",
    "# mobilenet_model_with_att = VQAModel(\n",
    "#     mobilenet, \n",
    "#     in_features_mobilenet, \n",
    "#     qu_vocab_size, \n",
    "#     ans_vocab_size,\n",
    "#     word_embed, \n",
    "#     hidden_size, \n",
    "#     num_hidden, \n",
    "#     feature_size, \n",
    "#     with_att=True,       \n",
    "#     is_train_image=False  \n",
    "# )\n",
    "\n",
    "# checkpoint_path = '/kaggle/input/dl_demo_mb_na/pytorch/default/1/best_mobile_net (3).pt'\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# mobilenet_model_with_att.load_state_dict(checkpoint['model_state_dict'])\n",
    "# mobilenet_model_with_att.to(device)\n",
    "# mobilenet_model_with_att.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# resnet.fc = nn.Identity()\n",
    "# in_features_resnet = 2048   \n",
    "\n",
    "# resnet_model_no_att = VQAModel(\n",
    "#     img_model=resnet,\n",
    "#     img_in_features=in_features_resnet,\n",
    "#     qu_vocab_size=qu_vocab_size,\n",
    "#     ans_vocab_size=ans_vocab_size,\n",
    "#     word_embed=word_embed,\n",
    "#     hidden_size=hidden_size,\n",
    "#     num_hidden=num_hidden,\n",
    "#     feature_size=feature_size,\n",
    "#     with_att=False,            \n",
    "#     is_train_image=False,\n",
    "#     pad_idx=PAD_IDX\n",
    "# )\n",
    "# checkpoint_path = '/kaggle/input/dl_demo_rn_na/pytorch/default/1/best_resnet_no_att.pt'\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# resnet_model_no_att.load_state_dict(checkpoint['model_state_dict'])\n",
    "# resnet_model_no_att.to(device)\n",
    "# resnet_model_no_att.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def tokenize_question(question, ques_vocab, max_qu_len=20):\n",
    "#     tokens = [ques_vocab.get(word, ques_vocab.get(\"<unk>\", 0)) for word in question.lower().split()]\n",
    "#     if len(tokens) > max_qu_len:\n",
    "#         tokens = tokens[:max_qu_len]\n",
    "#     else:\n",
    "#         tokens += [0] * (max_qu_len - len(tokens))\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def idx_to_answer(index, ans_vocab_path):\n",
    "#     with open(ans_vocab_path, 'r', encoding='utf-8') as f:\n",
    "#         vocab = f.read().splitlines()\n",
    "#     if 0 <= index < len(vocab):\n",
    "#         return vocab[index]\n",
    "#     else:\n",
    "#         return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def visualize_result(image_path, question, predicted_answer):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.gca().add_patch(patches.Rectangle((0, 0), image.width, 100, linewidth=0, facecolor='black', alpha=0.6))\n",
    "#     plt.text(5, 20, f\"Cu hi: {question}\", color='white', fontsize=12, verticalalignment='top')\n",
    "#     plt.text(5, 60, f\"D on: {predicted_answer}\", color='yellow', fontsize=12, verticalalignment='top')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def implement(model, image_path, ques_vocab_path, ans_vocab_path, transform, device, max_qu_len=20, max_ans_len=20):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     ques_vocab = load_vocab(ques_vocab_path)\n",
    "#     with open(ans_vocab_path, 'r', encoding='utf-8') as f:\n",
    "#         ans_vocab = f.read().splitlines()\n",
    "#     ans_idx_to_word = {idx: word for idx, word in enumerate(ans_vocab)}\n",
    "#     ans_word_to_idx = {word: idx for idx, word in enumerate(ans_vocab)}\n",
    "\n",
    "#     pad_token_id = ques_vocab.get('<pad>', 0)\n",
    "#     unk_token_id = ques_vocab.get('<unk>', 1)\n",
    "#     start_token_id = ans_word_to_idx.get('<start>', 1)\n",
    "#     end_token_id = ans_word_to_idx.get('<end>', 2)\n",
    "\n",
    "#    \n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "#     question = input(\"Nhp cu hi: \").strip().lower()\n",
    "#     tokens = question.split()\n",
    "#     question_ids = [ques_vocab.get(token, unk_token_id) for token in tokens]\n",
    "#     question_ids = question_ids[:max_qu_len] + [pad_token_id] * max(0, max_qu_len - len(question_ids))\n",
    "#     question_tensor = torch.tensor(question_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "#     answer_input = [start_token_id]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(max_ans_len):\n",
    "#             input_tensor = torch.tensor(answer_input, dtype=torch.long).unsqueeze(0).to(device)\n",
    "#             output = model(image_tensor, question_tensor, input_tensor)  # [1, seq_len, vocab_size]\n",
    "#             next_token_logits = output[0, -1]  # ly ra token cui cng\n",
    "#             next_token = torch.argmax(next_token_logits).item()\n",
    "\n",
    "#             if next_token == end_token_id:\n",
    "#                 break\n",
    "\n",
    "#             answer_input.append(next_token)\n",
    "\n",
    "#     predicted_ids = answer_input[1:]\n",
    "#     predicted_words = []\n",
    "#     for idx in predicted_ids:\n",
    "#         word = ans_idx_to_word.get(idx, \"<unk>\")\n",
    "#         if word in [\"<pad>\", \"<eos>\"]:\n",
    "#             break\n",
    "#         predicted_words.append(word)\n",
    "\n",
    "#     predicted_answer = ' '.join(predicted_words)\n",
    "\n",
    "#     print(\"\\n===== KT QU D ON =====\")\n",
    "#     print(\"Cu hi:\", question)\n",
    "#     print(\"Tr li:\", predicted_answer)\n",
    "\n",
    "#     visualize_result(image_path, question, predicted_answer)\n",
    "#     return predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ques_vocab_path = \"/kaggle/working/question_vocabs.txt\"\n",
    "# ans_vocab_path = \"/kaggle/working/answer_vocabs.txt\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# image_path = \"/kaggle/input/vqa-test/data/places/id_751.png\"\n",
    "# predicted_answer = implement(resnet_model_no_att, image_path, ques_vocab_path, ans_vocab_path, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6865705,
     "sourceId": 11025236,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6966072,
     "sourceId": 11163401,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 283404,
     "modelInstanceId": 262279,
     "sourceId": 308335,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
