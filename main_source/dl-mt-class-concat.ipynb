{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resize_images(input_dir, output_dir, size=(224, 224)):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for img_file in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_file)\n",
    "        save_path = os.path.join(output_dir, img_file)\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize(size, Image.LANCZOS) \n",
    "                img.save(save_path, img.format)\n",
    "        except Exception as e:\n",
    "            print(f\"Wrong at {img_file}: {e}\")\n",
    "\n",
    "    print(f\"Resized: {len(os.listdir(output_dir))} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/kaggle/input/vqa-trainv2/data/places\"\n",
    "output_dir = \"/kaggle/working/resized_images_train\"\n",
    "resize_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/kaggle/input/vqa-test/data/places\"\n",
    "output_dir = \"/kaggle/working/resized_images_test\"\n",
    "resize_images(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    regex = re.compile(r'(\\W+)')\n",
    "    tokens = regex.split(sentence.lower())\n",
    "    return [w.strip() for w in tokens if len(w.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_ans_vocab(annotation_file, save_path=\"/kaggle/working/answer_vocabs.txt\"):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)['annotations']\n",
    "\n",
    "    answers = set()\n",
    "    for ann in annotations:\n",
    "        for ans in ann['answers']:\n",
    "            answers.add(ans['answer'])\n",
    "\n",
    "    answers = sorted(answers) \n",
    "    answers.insert(0, '<unk>')  \n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines([ans + '\\n' for ans in answers])\n",
    "    print(f\" Generated answer vocab with {len(answers)} unique answers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "annotation_file = \"/kaggle/input/vqa-trainv2/data/annotations.json\"\n",
    "make_ans_vocab(annotation_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_q_vocab(question_file, save_path=\"/kaggle/working/question_vocabs.txt\"):\n",
    "    with open(question_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    words = set()\n",
    "    for q in data['questions']:\n",
    "        words.update(tokenizer(q['question']))\n",
    "\n",
    "    words = sorted(words)\n",
    "    words.insert(0, '<pad>')\n",
    "    words.insert(1, '<unk>')\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.writelines([w + '\\n' for w in words])\n",
    "\n",
    "    print(f\"Generated question vocab: {len(words)} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question_file = \"/kaggle/input/vqa-trainv2/data/questions.json\"\n",
    "make_q_vocab(question_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "    return {word: idx for idx, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_image_file(image_id, image_dir):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.startswith(f\"id_{image_id}.\"):\n",
    "            return os.path.join(image_dir, filename) \n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_data(question_file, annotation_file, image_dir, output_path, labeled=True):\n",
    "    with open(question_file, 'r', encoding=\"utf-8\") as f:\n",
    "        questions = json.load(f)['questions']\n",
    "\n",
    "    if labeled:\n",
    "        with open(annotation_file, 'r', encoding=\"utf-8\") as f:\n",
    "            annotations = json.load(f)['annotations']\n",
    "        q_dict = {ann['question_id']: ann for ann in annotations}\n",
    "\n",
    "    vocab2idx = load_vocab(\"/kaggle/working/question_vocabs.txt\")  \n",
    "    ans_vocab2idx = load_vocab(\"/kaggle/working/answer_vocabs.txt\")  \n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for q in questions:\n",
    "        qu_id = q['question_id']\n",
    "        img_id = q['image_id']\n",
    "        qu_sentence = q['question']\n",
    "        qu_tokens = tokenizer(qu_sentence)\n",
    "        qu2idx = [vocab2idx.get(token, vocab2idx['<unk>']) for token in qu_tokens]  \n",
    "\n",
    "        img_path = find_image_file(img_id, image_dir)\n",
    "\n",
    "        info = {\n",
    "            'img_name': os.path.basename(img_path),\n",
    "            'img_path': img_path,\n",
    "            'question': qu_sentence,\n",
    "            'qu_tokens': qu2idx,\n",
    "            'qu_id': qu_id\n",
    "        }\n",
    "\n",
    "        if labeled:\n",
    "            annotation_ans = q_dict[qu_id]['answers']\n",
    "            valid_ans = [ans['answer'] for ans in annotation_ans]\n",
    "            ans_idx = ans_vocab2idx.get(valid_ans[0], ans_vocab2idx['<unk>'])  \n",
    "            info['answer'] = valid_ans[0]\n",
    "            info['ans_token'] = ans_idx  # Lưu dưới dạng số nguyên\n",
    "\n",
    "        dataset.append(info)\n",
    "\n",
    "\n",
    "    with open(output_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4, ensure_ascii=False)  \n",
    "\n",
    "    print(f\"Saved JSON at {output_path} | Total samples: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_dir = \"/kaggle/working/resized_images_train\"\n",
    "question_file = \"/kaggle/input/vqa-trainv2/data/questions.json\"\n",
    "annotation_file = \"/kaggle/input/vqa-trainv2/data/annotations.json\"\n",
    "\n",
    "process_data(question_file, annotation_file, image_dir, '/kaggle/working/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_dir = \"/kaggle/working/resized_images_test\"\n",
    "question_file = \"/kaggle/input/vqa-test/data/questions.json\"\n",
    "annotation_file = \"/kaggle/input/vqa-test/data/annotations.json\"\n",
    "\n",
    "process_data(question_file, annotation_file, image_dir, '/kaggle/working/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, json_path, transform=None, max_qu_len=20, typeData='train'):\n",
    "        self.max_qu_len = max_qu_len\n",
    "        self.transform = transform\n",
    "        self.typeData = typeData\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.data = []\n",
    "        if self.typeData == 'train':\n",
    "            for i in range(0, len(data), 210):\n",
    "                self.data.extend(data[i:i+200])  \n",
    "        elif self.typeData == 'valid':\n",
    "            for i in range(0, len(data), 210):\n",
    "                self.data.extend(data[i+200:i+210])  \n",
    "        elif self.typeData == 'test':\n",
    "            self.data = data  \n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        img_path = sample[\"img_path\"]\n",
    "        question_tokens = torch.tensor(sample[\"qu_tokens\"], dtype=torch.long)\n",
    "        answer_token = torch.tensor(sample[\"ans_token\"], dtype=torch.long)\n",
    "\n",
    "        if len(question_tokens) > self.max_qu_len:\n",
    "            question_tokens = question_tokens[:self.max_qu_len]\n",
    "        else:\n",
    "            padding = torch.zeros(self.max_qu_len - len(question_tokens), dtype=torch.long)\n",
    "            question_tokens = torch.cat([question_tokens, padding])\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, question_tokens, answer_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self, model, in_features, feature_size, is_train=False):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(in_features, feature_size)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def forward(self, image):\n",
    "        if self.is_train:\n",
    "            img_feature = self.model(image)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                img_feature = self.model(image)\n",
    "\n",
    "        img_feature = self.fc(img_feature)\n",
    "        l2_norm = F.normalize(img_feature, p=2, dim=1)\n",
    "        return l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1)  \n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)  \n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)  \n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class QuEncoder(nn.Module):\n",
    "    def __init__(self, qu_vocab_size, word_embed, hidden_size, num_hidden, qu_feature_size, with_att=False):\n",
    "        super(QuEncoder, self).__init__()\n",
    "        self.with_att = with_att\n",
    "        self.word_embedding = nn.Embedding(qu_vocab_size, word_embed)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(word_embed, hidden_size, num_hidden, batch_first=True)\n",
    "\n",
    "        if self.with_att:\n",
    "            self.attention = Attention(hidden_size)\n",
    "            self.fc = nn.Linear(hidden_size, qu_feature_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2 * num_hidden * hidden_size, qu_feature_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        qu_embedding = self.word_embedding(question)\n",
    "        qu_embedding = self.tanh(qu_embedding)\n",
    "        lstm_out, (hidden, cell) = self.lstm(qu_embedding)\n",
    "\n",
    "        if self.with_att:\n",
    "            attn_output = self.attention(lstm_out)\n",
    "        else:\n",
    "            qu_feature = torch.cat((hidden, cell), dim=2)\n",
    "            qu_feature = qu_feature.transpose(0, 1).reshape(qu_feature.size(1), -1)\n",
    "            attn_output = self.tanh(qu_feature)\n",
    "\n",
    "        qu_feature = self.fc(attn_output)\n",
    "        return qu_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, img_model, img_in_features, qu_vocab_size, ans_vocab_size, word_embed, hidden_size, num_hidden, feature_size, with_att=False, is_train_image=False):\n",
    "        super(VQAModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.img_encoder = ImgEncoder(img_model, img_in_features, feature_size, is_train_image)\n",
    "        self.qu_encoder = QuEncoder(qu_vocab_size, word_embed, hidden_size, num_hidden, feature_size, with_att)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_size * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, ans_vocab_size)  \n",
    "\n",
    "    def forward(self, image, question):\n",
    "\n",
    "        img_feature = self.img_encoder(image)  \n",
    "        qst_feature = self.qu_encoder(question)  \n",
    "        \n",
    "        combined_feature = torch.cat([img_feature, qst_feature], dim=1)  \n",
    "\n",
    "\n",
    "        combined_feature = self.dropout(combined_feature)\n",
    "        combined_feature = self.tanh(combined_feature)\n",
    "\n",
    "        combined_feature = self.fc1(combined_feature)  \n",
    "        combined_feature = self.dropout(combined_feature)\n",
    "        combined_feature = self.tanh(combined_feature)\n",
    "\n",
    "        logits = self.fc2(combined_feature)  \n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_accuracy']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['valid_accuracy'], label='Valid Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['valid_loss'], label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=10, name=\"default\", patience=10):\n",
    "    model.to(device)\n",
    "    best_valid_accuracy = 0.0\n",
    "    epochs_no_improve = 0  \n",
    "\n",
    "    history = {'train_loss': [], 'valid_loss': [], 'train_accuracy': [], 'valid_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, questions, answers in train_dataloader:\n",
    "            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion(outputs, answers)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += answers.size(0)\n",
    "            correct += (predicted == answers).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, questions, answers in valid_dataloader:\n",
    "                images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "\n",
    "                outputs = model(images, questions)\n",
    "                loss = criterion(outputs, answers)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                valid_total += answers.size(0)\n",
    "                valid_correct += (predicted == answers).sum().item()\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_dataloader)\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['valid_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]:\"\n",
    "              f\" Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% |\"\n",
    "              f\" Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "        # Save last checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'valid_accuracy': valid_accuracy,\n",
    "            'valid_loss': valid_loss\n",
    "        }, f\"last_{name}.pt\")\n",
    "\n",
    "        # Save best checkpoint and check early stopping\n",
    "        if valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = valid_accuracy\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_accuracy': valid_accuracy,\n",
    "                'valid_loss': valid_loss\n",
    "            }, f\"best_{name}.pt\")\n",
    "\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation accuracy: {valid_accuracy:.2f}%\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}, no improvement in validation accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, questions, answers in test_dataloader:\n",
    "            images, questions = images.to(device), questions.to(device)\n",
    "\n",
    "            outputs = model(images, questions)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(answers.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "    print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "json_path_train = \"/kaggle/working/train.json\"  \n",
    "json_path_test = \"/kaggle/working/test.json\"  \n",
    "\n",
    "train_dataset = VQADataset(json_path_train, max_qu_len=20, typeData='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "valid_dataset = VQADataset(json_path_train, max_qu_len=20, typeData='valid')\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = VQADataset(json_path_test, max_qu_len=20, typeData='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_size = 512\n",
    "qu_vocab_size = 186\n",
    "ans_vocab_size = 33\n",
    "word_embed = 128\n",
    "hidden_size = 256\n",
    "num_hidden = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenet.classifier = nn.Identity()  \n",
    "in_features_mobilenet = 1280  \n",
    "\n",
    "mobilenet_model = VQAModel(mobilenet, \n",
    "                           in_features_mobilenet, \n",
    "                           qu_vocab_size, \n",
    "                           ans_vocab_size,\n",
    "                           word_embed, \n",
    "                           hidden_size, \n",
    "                           num_hidden, \n",
    "                           feature_size, \n",
    "                           with_att=True, \n",
    "                           is_train_image=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam( mobilenet_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(mobilenet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='mobile_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(mobilenet_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet.fc = nn.Identity()\n",
    "in_features_resnet = 2048   \n",
    "\n",
    "resnet_model = VQAModel(resnet, \n",
    "                        in_features_resnet, \n",
    "                        qu_vocab_size, \n",
    "                        ans_vocab_size,\n",
    "                        word_embed, \n",
    "                        hidden_size, \n",
    "                        num_hidden, \n",
    "                        feature_size,  \n",
    "                        with_att=True, \n",
    "                        is_train_image=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(resnet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(resnet_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "efficientnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "efficientnet.classifier = nn.Identity()\n",
    "\n",
    "in_features_efficientnet = 1536\n",
    "\n",
    "efficientnet_model = VQAModel(efficientnet, \n",
    "                              in_features_efficientnet, \n",
    "                              qu_vocab_size, \n",
    "                              ans_vocab_size,\n",
    "                              word_embed, \n",
    "                              hidden_size, \n",
    "                              num_hidden, \n",
    "                              feature_size,  \n",
    "                              with_att=True, \n",
    "                              is_train_image=False )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficientnet_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(efficientnet_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='efficientnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(efficientnet_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "mobilenet.classifier = nn.Identity()  \n",
    "in_features_mobilenet = 1280  \n",
    "\n",
    "mobilenet_model_no_att = VQAModel(mobilenet, \n",
    "                                  in_features_mobilenet, \n",
    "                                  qu_vocab_size, \n",
    "                                  ans_vocab_size,\n",
    "                                  word_embed, \n",
    "                                  hidden_size, \n",
    "                                  num_hidden, \n",
    "                                  feature_size, \n",
    "                                  with_att=False, \n",
    "                                  is_train_image=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam( mobilenet_model_no_att.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(mobilenet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='mobile_net_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(mobilenet_model_no_att, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet.fc = nn.Identity()\n",
    "in_features_resnet = 2048   \n",
    "\n",
    "resnet_model_no_att = VQAModel(resnet, \n",
    "                               in_features_resnet, \n",
    "                               qu_vocab_size, \n",
    "                               ans_vocab_size,\n",
    "                               word_embed, \n",
    "                               hidden_size, \n",
    "                               num_hidden, \n",
    "                               feature_size, \n",
    "                               with_att=False, \n",
    "                               is_train_image=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model_no_att.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(resnet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='resnet_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(resnet_model_no_att, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "efficientnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "efficientnet.classifier = nn.Identity()\n",
    "\n",
    "in_features_efficientnet = 1536\n",
    "\n",
    "efficientnet_model_no_att = VQAModel(efficientnet, \n",
    "                                     in_features_efficientnet, \n",
    "                                     qu_vocab_size, \n",
    "                                     ans_vocab_size,\n",
    "                                     word_embed, \n",
    "                                     hidden_size, \n",
    "                                     num_hidden, \n",
    "                                     feature_size, \n",
    "                                     with_att=False, \n",
    "                                     is_train_image=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(efficientnet_model_no_att.parameters(), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(efficientnet_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='efficientnet_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(efficientnet_model_no_att, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FromScarth Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNN_Self_Build(nn.Module):\n",
    "    def __init__(self, feature_dim=3096, dropout_rate=0.5):\n",
    "        super(CNN_Self_Build, self).__init__()\n",
    "        #\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, feature_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)   \n",
    "        x = self.block2(x)   \n",
    "        x = self.block3(x)   \n",
    "        x = self.block4(x)   \n",
    "        x = self.global_avg_pool(x)  \n",
    "        x = x.view(x.size(0), -1)      \n",
    "        features = self.fc(x)          \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "in_features_cnn_self_build = 2048\n",
    "\n",
    "cnn_model  = VQAModel(cnn_self_build, \n",
    "                      in_features_cnn_self_build, \n",
    "                      qu_vocab_size, \n",
    "                      ans_vocab_size,\n",
    "                      word_embed, \n",
    "                      hidden_size, \n",
    "                      num_hidden, \n",
    "                      feature_size,  \n",
    "                      with_att=True, \n",
    "                      is_train_image=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train(cnn_model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluate_test(cnn_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "in_features_cnn_self_build = 2048\n",
    "\n",
    "cnn_model_no_att  = VQAModel(cnn_self_build, \n",
    "                             in_features_cnn_self_build, \n",
    "                             qu_vocab_size, \n",
    "                             ans_vocab_size,\n",
    "                             word_embed, \n",
    "                             hidden_size, \n",
    "                             num_hidden, \n",
    "                             feature_size, \n",
    "                             with_att=False, \n",
    "                             is_train_image=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model_no_att.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train(cnn_model_no_att, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs=epochs, name='default_no_att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_test(cnn_model_no_att, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# cnn_self_build = CNN_Self_Build(feature_dim=2048, dropout_rate=0.5)\n",
    "\n",
    "# in_features_cnn_self_build = 2048\n",
    "# cnn_model_no_att  = VQAModel(cnn_self_build, \n",
    "#                              in_features_cnn_self_build, \n",
    "#                              qu_vocab_size, \n",
    "#                              ans_vocab_size,\n",
    "#                              word_embed, \n",
    "#                              hidden_size, \n",
    "#                              num_hidden, \n",
    "#                              feature_size, \n",
    "#                              with_att=False, \n",
    "#                              is_train_image=True)\n",
    "\n",
    "# checkpoint = torch.load('/kaggle/input/demo_dl_mt/pytorch/default/1/best_default_no_att.pt', map_location=device)\n",
    "# cnn_model_no_att.load_state_dict(checkpoint['model_state_dict'])\n",
    "# cnn_model_no_att.to(device)\n",
    "# cnn_model_no_att.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def tokenize_question(question, ques_vocab, max_qu_len=20):\n",
    "#     tokens = [ques_vocab.get(word, ques_vocab.get(\"<unk>\", 0)) for word in question.lower().split()]\n",
    "#     if len(tokens) > max_qu_len:\n",
    "#         tokens = tokens[:max_qu_len]\n",
    "#     else:\n",
    "#         tokens += [0] * (max_qu_len - len(tokens))\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def idx_to_answer(index, ans_vocab_path):\n",
    "#     with open(ans_vocab_path, 'r', encoding='utf-8') as f:\n",
    "#         vocab = f.read().splitlines()\n",
    "#     if 0 <= index < len(vocab):\n",
    "#         return vocab[index]\n",
    "#     else:\n",
    "#         return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def visualize_result(image_path, question, predicted_answer):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Vẽ box chứa text\n",
    "#     plt.gca().add_patch(patches.Rectangle((0, 0), image.width, 100, linewidth=0, facecolor='black', alpha=0.6))\n",
    "#     plt.text(5, 20, f\"Câu hỏi: {question}\", color='white', fontsize=12, verticalalignment='top')\n",
    "#     plt.text(5, 60, f\"Dự đoán: {predicted_answer}\", color='yellow', fontsize=12, verticalalignment='top')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def implement(model, image_path, ques_vocab_path, ans_vocab_path, transform, device):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     ques_vocab = load_vocab(ques_vocab_path)\n",
    "\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "#     question = input(\"Nhập câu hỏi: \")\n",
    "#     question_tokens = tokenize_question(question, ques_vocab)\n",
    "#     question_tensor = torch.tensor(question_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(image_tensor, question_tensor)\n",
    "#         predicted_answer_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "#     predicted_answer = idx_to_answer(predicted_answer_idx, ans_vocab_path)\n",
    "#     print(f\"Dự đoán của mô hình: {predicted_answer}\")\n",
    "\n",
    "#     visualize_result(image_path, question, predicted_answer)\n",
    "\n",
    "#     return predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def implement_random_samples(model, test_json_path, ques_vocab_path, ans_vocab_path, transform, device, sample_size=20):\n",
    "#     with open(test_json_path, 'r', encoding='utf-8') as f:\n",
    "#         test_data = json.load(f)\n",
    "    \n",
    "#     selected_samples = random.sample(test_data, sample_size)\n",
    "\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     ques_vocab = load_vocab(ques_vocab_path)\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     for entry in selected_samples:\n",
    "#         img_path = entry['img_path']\n",
    "#         question = entry['question']\n",
    "\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "#         question_tokens = tokenize_question(question, ques_vocab)\n",
    "#         question_tensor = torch.tensor(question_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = model(image_tensor, question_tensor)\n",
    "#             predicted_answer_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "#         predicted_answer = idx_to_answer(predicted_answer_idx, ans_vocab_path)\n",
    "\n",
    "#         results.append({\n",
    "#             'image_path': img_path,\n",
    "#             'question': question,\n",
    "#             'predicted_answer': predicted_answer\n",
    "#         })\n",
    "\n",
    "\n",
    "#     cols = 5\n",
    "#     rows = 4\n",
    "\n",
    "#     plt.figure(figsize=(20, 16))\n",
    "\n",
    "#     for i, res in enumerate(results):\n",
    "#         img = Image.open(res['image_path']).convert(\"RGB\")\n",
    "#         plt.subplot(rows, cols, i + 1)\n",
    "#         plt.imshow(img)\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         plt.gca().add_patch(patches.Rectangle((0, 0), img.width, 60, linewidth=0, facecolor='black', alpha=0.6))\n",
    "#         plt.text(5, 20, f\"Q: {res['question']}\", color='white', fontsize=8, verticalalignment='top')\n",
    "#         plt.text(5, 45, f\"A: {res['predicted_answer']}\", color='yellow', fontsize=8, verticalalignment='top')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ques_vocab_path = \"/kaggle/working/question_vocabs.txt\"\n",
    "# ans_vocab_path = \"/kaggle/working/answer_vocabs.txt\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# image_path = \"/kaggle/input/vqa-test/data/places/id_751.png\"\n",
    "# predicted_answer = implement(cnn_model_no_att, image_path, ques_vocab_path, ans_vocab_path, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_json_path = '/kaggle/working/test.json'\n",
    "\n",
    "# implement_random_samples(cnn_model_no_att, test_json_path, ques_vocab_path, ans_vocab_path, transform, device, sample_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6865705,
     "sourceId": 11025236,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6906049,
     "sourceId": 11080418,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
